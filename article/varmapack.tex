% Local Variables:
% reftex-default-bibliography: ("varmapack.bib")
% End:
\documentclass[a4paper,12pt]{extarticle} % Hægt að nota 10 pt, 11 pt, report
\usepackage{snidmat}

\usepackage{algorithm}
\usepackage{algpseudocode}

\renewcommand{\alglinenumber}[1]{\small\rmfamily #1}

% \usepackage[ruled,vlined]{algorithm2e}

\author{Kristján Jónasson}
\date{Nov. 2025}
\title{Burn-in Free Simulation of VARMA Time Series}

\begin{document}

\maketitle
\section{Introduction}
\section{Covariance structure of VARMA time series}
This section builds on the articles by \citet{jonasson2008method} and
\citet{jonasson2008algorithm} referred to below as the \emph{method paper} and the
\emph{algorithm paper} below (this is TOMS terminology). The notation of these
articles is largely followed, with minor adjustments. Let $x_t \in
\mathbb{R}^r$ follow a zero-mean, stationary $\mathrm{VARMA}(p,q)$ process:
%
\begin{equation}
  \label{eq:varma}
  x_t
  = \sum_{j=1}^{p} A_j x_{t-j}
  + \eps_t
  + \sum_{j=1}^{q} B_j \eps_{t-j},
\end{equation}
%
where $\eps_t$ denotes $\mathcal{N}(0, \Sigma)$ white noise shocks, and the $x_t$
are called states. Let $S_j = \Cov(x_t, x_{t-j})$ and $C_j = \Cov(x_t,
\eps_{t-j})$. $S_0,\ldots,S_{p-1}$ are obtained by solving the so-called
vector-Yule-Walker equations as described in Appendix B of
\citet{jonasson2006efficient}, and the $C_j$ are given by $C_0 = \Sigma$ and
% 
\begin{equation}
  C_j = A_1C_{j-1} + \cdots + A_j C_0 + B_j\Sigma \quad (j=1,2,\ldots),
\end{equation}
%
where $A_i$ and $B_j$ are taken as $0$ for $i>p$ and $j>q$, respectively. Note
that there is a mistake in eq. (7) in the method article where the last term
should be $B_j\Sigma$, not $B_q\Sigma$. To simulate a VARMA time series
\eqref{eq:varma} for $t=1,\ldots,n$, the shocks can be obtained first, simply by
drawing $\eps_t$ from $\mathcal{N}(0, \Sigma)$ $n$ times. The states, however,
are interdependent and also depend on the shocks. Because each state depends
only on the previous $p$ states and $q$ shocks, it is sufficient to sample
only the first $h = \max(p,q)$ states from their joint distribution, the
remaining states can be obtained deterministically with \eqref{eq:varma}.

To fill in some details, denote the stacked vector of $x_1,\ldots,x_h$ with
$x_{1:h}$ and similarly for $\eps$. Let $S$, $C$, and $\Theta$ denote,
respectively, $\Var x_{1:h}$, $\Cov(x_{1:h}, \eps_{1:h})$, and $\Var
\eps_{1:h}$. Then $S$ is a block Toeplitz matrix with
$(i,j)$-block
%
\begin{equation}
  S_{ij} =
  \begin{cases}
    S_{i-j} & \text{for } i\geq j\\
    S_{j-i}^\top & \text{for } i<j,\\
  \end{cases}
\end{equation}
%
cf. eq. (6) in the method article, $C$ is block lower-triangular with $(i,j)$
block
%
\begin{equation}
  C_{ij} =
  \begin{cases}
    C_{i-j} & \text{for } i\geq j\\
    0 & \text{for } i<j\\
  \end{cases}
\end{equation}
%
and $\Theta$ is block diagonal with all $h$ blocks equal to $\Sigma$. All three
matrices are $rh \times rh$. Since $x_{1:h}$ and $\eps_{1:h}$ are jointly
Gaussian, the conditional distribution of $x_{1:h}$ given $\eps_{1:h}$ is normal
with mean and variance
%
\begin{equation}
  \label{eq:eR1}
e = C\Theta^{-1}\eps_{1:h}\quad\text{and}\quad
R = S - C\Theta^{-1}C^\top.  
\end{equation}
%
Simpler formulae for $e$ and $R$ are obtained by setting $\Psi = C\Theta^{-1}$,
which like $C$ is a lower triangular block matrix. Block $i,j$, $\Psi_{i-j}$ can
be computed by setting $\Psi_0 = I$ and computing $\Psi_1,\ldots,\Psi_{h-1}$
with the recurrence:
%
\begin{equation}
  \label{eq:psi-j}
  \Psi_j = A_1\Psi_{j-1} + \cdots + A_j\Psi_0 + B_j
\end{equation}
where as before $A_i$ and $B_j$ are taken as $0$ for $i>p$, $j>q$. Then
%
\begin{equation}
  \label{eq:eR2}
  e = \Psi\eps_{1:h}\quad\text{and}\quad
  R = S - \Psi\Theta \Psi^\top.  
\end{equation}
% 
$\Psi_j$ is actually the $j$-th coefficient of the series obtained by rewriting
\eqref{eq:varma} in pure moving-average form.

\section{Simulation}
Consider now the generation of several simulated replicates of $x_{1:n}$. To
begin with the conditional covariance matrix $R$ should be calculated along with
its Cholesky factor, $L_R$, as well as that of $\Sigma$, $L_\Sigma$. The
Cholesky factor of $\Theta$ is block diagonal with blocks $L_\Sigma$, and it is
most efficient to compute $\hat{\Psi} = \Psi L_\Theta$ first. Then dsyrk from the
level-3 BLAS can be used to form $R = S - \hat{\Psi} \hat{\Psi}^\top$. Assume $M$
replicates are desired, consisting of an $hr \times M$ matrix $X$ with the first
$h$ terms of the simulated series, and an $(n-h) \times M$ matrix $Y$ with the
remaining terms. The following algorithm can then be followed:

\begin{enumerate}
\item Fill an $rh \times M$ matrix $E$ with replicates of $\eps_t$ drawn from $\N(0,\Sigma)$.
\item Let $E:=\Psi E$ (then its columns contain conditional means, $e$)
\item Fill $X$ with multivariate normals drawn from $\N(0, R)$.
\item Let $X := E + X$.
\item Use \eqref{eq:varma} to compute the remaining states of each replicate.
\end{enumerate}

Step 3 involves multiplying a matrix of $\N(0,I)$ columns with $L_R$ and for
that dtrmm from the level-3 BLAS can be used. Level-3 BLAS (dgemm) can also be
used in step 5 by multiplying relevant rows of $X$ and/or $Y$ with matrices $A =
[A_p,\ldots,A_1]$ and $B = [B_q,\ldots,B_1]$.

The 2008 algorithm paper takes a somewhat different but mathematically
equivalent approach. Instead of drawing the first $h$ shocks first, the first
$h$ states are drawn from $\N(0, S)$ and then the conditional distribution of
$\eps_{1:h}$ given $x_{1:h}$ is used to draw the first $h$ shocks. When looked
at more closely, that method turns out to be more complex and more expensive
than the current one.

\section{Notes}
The R package \texttt{ts.extend} described by \citet{oneill2021tsextend}
implements burn-in-free simulation of scalar ARMA processes via conditional
distributions, closely related to the method used here. The package was formerly
available in the CRAN repository but is now accessible on Github and in the CRAN archive.

\citet{giurcanu2015} discusses the simulation of non-causal VARMA processes,
where x_t depends on both past and future innovations. His method requires a
burn-in period.

Burn-in is an important consideration in time-series simulation and analysis.
\citet{li2024burnin} study this issue for general stationary linear, non-VARMA
processes, including long-memory models, where an exact burn-in-free simulation
may not be feasible and a burn-in period must be accepted.

\citet{boubacar2024varma} also examine non-Gaussian models, focusing on weak
VARMA processes in which the innovations are uncorrelated but not independent.
Their Monte Carlo study uses simulated VARMA data but does not discuss the
simulation procedure or burn-in. Nevertheless, their work illustrates continuing
interest in the estimation and diagnostic testing of VARMA models, where the
availability of burn-in-free simulation methods could be useful.

Popular Python libraries such as statsmodels and PyFlux provide estimation and
forward simulation for VAR and VARMA models, typically by means of a state-space
representation. However, these tools simulate conditionally on prespecified
initial states and do not appear to offer an option to draw the initial h
observations from the exact stationary distribution.

\paragraph{Note on VARMAX}
The inclusion of deterministic or fixed exogenous variables $u_t$ leads to the
$\mathrm{VARMAX}(p,q,s)$ extension
\begin{equation}
  x_t
  = \sum_{j=1}^{p} A_j x_{t-j}
  + \sum_{j=0}^{s} D_j u_{t-j}
  + \varepsilon_t
  + \sum_{j=1}^{q} B_j \varepsilon_{t-j}.
\end{equation}
As the exogenous sequence $u_t$ is assumed fixed, it does not affect the
covariance structure of the process, and all results derived above for the
$\mathrm{VARMA}$ case remain valid.

        package  count
1 beyondWhittle  28888
2       bigtime  29541
3           ldt   9534
4           MTS 154200
5     multiwave  17409


Recent work (e.g. Wilms et al., 2021) shows that parsimonious or sparsely
estimated VARMA models can outperform higher-order VAR alternatives in
predictive accuracy, especially when moving-average effects are present.Use
r=16, 17, and 168 in their examples.

\bibliographystyle{plainnat}   % or "apalike" if you prefer
\bibliography{varmapack}

\end{document}
